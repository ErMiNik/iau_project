{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Realizácia predspracovania dát (5b)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (A-1b) Dáta si rozdeľte na trénovaciu a testovaciu množinu podľa vami preddefinovaného pomeru. Ďalej pracujte len **s trénovacím datasetom**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "folder = Path(\"./070\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nacitame data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections_df = pd.read_csv(folder / \"connections.csv\", delimiter=\"\\t\", parse_dates=['ts'])\n",
    "devices_df = pd.read_csv(folder / \"devices.csv\", delimiter=\"\\t\")\n",
    "processes_df = pd.read_csv(folder / \"processes.csv\", delimiter=\"\\t\", parse_dates=['ts'])\n",
    "profiles_df = pd.read_csv(folder / \"profiles.csv\", delimiter=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zlucenie dat z roznych tabuliek\n",
    "\n",
    "vieme ze vsetky tabulky maju spolocny atribut imei, ten ale nie je unikatny takze iba na zaklade jeho nemozme tabulky spajat.\n",
    "\n",
    "unikatny kluc podla ktoreho by sme mohli tabulky spojit je imei + datetime + mwra. Ak spojime tabulky na zaklade tychto atributov dostaneme validny merge\n",
    "\n",
    "kedze tieto atributy najdeme len v datach connections.csv a processes.csv, budeme joinovat iba tieto tabulky"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data rozdelime pomocou funkcie train_test_split z sklearn.model v pomere 80:20, random_state zaistuje opakovatelnost aby sme mohli pripadne debugovat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15426, 33)\n"
     ]
    }
   ],
   "source": [
    "merged_data = pd.merge(connections_df, processes_df, on=['imei', 'ts', 'mwra'], how='inner')\n",
    "\n",
    "# este rozdelime cas na konkretne atributy, aby\n",
    "# sme ich pripadne mohli pouzit v ML\n",
    "\n",
    "# merged_data['year'] = merged_data['ts'].dt.year\n",
    "# merged_data['month'] = merged_data['ts'].dt.month\n",
    "# merged_data['day'] = merged_data['ts'].dt.day\n",
    "# merged_data['hour'] = merged_data['ts'].dt.hour\n",
    "# merged_data['day_of_week'] = merged_data['ts'].dt.dayofweek\n",
    "\n",
    "# tieto data sme sa rozhodli nepridavat (vid 2.2 A))\n",
    "\n",
    "print(merged_data.shape)\n",
    "merged_data.head()\n",
    "\n",
    "\n",
    "# potom splitnut merged data na 80 % trainning a 20% testing pomocou train_test_split\n",
    "merged_train_df, merged_test_df = train_test_split(merged_data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12340 entries, 3784 to 7270\n",
      "Data columns (total 33 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   ts                          12340 non-null  datetime64[ns]\n",
      " 1   imei                        12340 non-null  int64         \n",
      " 2   mwra                        12340 non-null  float64       \n",
      " 3   c.android.youtube           12340 non-null  float64       \n",
      " 4   c.katana                    12340 non-null  float64       \n",
      " 5   c.dogalize                  12340 non-null  float64       \n",
      " 6   c.android.chrome            12340 non-null  float64       \n",
      " 7   c.android.gm                12340 non-null  float64       \n",
      " 8   c.UCMobile.x86              12340 non-null  float64       \n",
      " 9   c.raider                    12340 non-null  float64       \n",
      " 10  c.android.vending           12340 non-null  float64       \n",
      " 11  c.updateassist              12340 non-null  float64       \n",
      " 12  c.UCMobile.intl             12340 non-null  float64       \n",
      " 13  p.android.chrome            12340 non-null  float64       \n",
      " 14  p.android.documentsui       12340 non-null  float64       \n",
      " 15  p.android.packageinstaller  12340 non-null  float64       \n",
      " 16  p.android.settings          12340 non-null  float64       \n",
      " 17  p.android.gm                12340 non-null  float64       \n",
      " 18  p.android.externalstorage   12340 non-null  float64       \n",
      " 19  p.system                    12340 non-null  float64       \n",
      " 20  p.android.gms               12340 non-null  float64       \n",
      " 21  p.dogalize                  12340 non-null  float64       \n",
      " 22  p.process.gapps             12340 non-null  float64       \n",
      " 23  p.katana                    12340 non-null  float64       \n",
      " 24  p.google                    12340 non-null  float64       \n",
      " 25  p.notifier                  12340 non-null  float64       \n",
      " 26  p.android.defcontainer      12340 non-null  float64       \n",
      " 27  p.android.vending           12340 non-null  float64       \n",
      " 28  p.inputmethod.latin         12340 non-null  float64       \n",
      " 29  p.simulator                 12340 non-null  float64       \n",
      " 30  p.olauncher                 12340 non-null  float64       \n",
      " 31  p.browser.provider          12340 non-null  float64       \n",
      " 32  p.gms.persistent            12340 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(31), int64(1)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "merged_train_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (B-1b) Transformujte dáta na vhodný formát pre ML t.j. jedno pozorovanie musí byť opísané jedným riadkom a každý atribút musí byť v numerickom formáte (encoding). Iteratívne integrujte aj kroky v predspracovaní dát z prvej fázy (missing values, outlier detection) ako celok. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ako prve si v mergnutom datasete urcime spravne datove typy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: osetrit typy ak treba (asi nebude treba tak len napisat ze to je osetrene)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifikujeme a vyriešime (odstránime) duplikáty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    # v datasetoch sa nevyskytol prípad, kde by bol duplikátny sĺpec, a preto tu riešime iba riadky\n",
    "\n",
    "    if len(df.loc[df.duplicated(), :]) > 0:\n",
    "        df_dupless = df.loc[~df.duplicated(), :]\n",
    "    \n",
    "    return df_dupless"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifikujeme a vyriešime chýbajúce hodnoty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_missing_values(df):\n",
    "    return df.isnull().any().any()\n",
    "\n",
    "\n",
    "def count_missing_values(col):\n",
    "    return col.isna().sum()\n",
    "\n",
    "\n",
    "def replace_missing_values(col):\n",
    "    # lets calculate the amount of outliers to determine, whether to use mean or median\n",
    "\n",
    "    Q1 = col.quantile(0.25)\n",
    "    Q3 = col.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # lets identify the outliers\n",
    "    outliers = ((col < (Q1 - 1.5 * IQR)) | (col > (Q3 + 1.5 * IQR)))\n",
    "    outliers_percentage = (outliers.sum() / col.count()) * 100\n",
    "\n",
    "    if outliers_percentage < 5:\n",
    "        return col.fillna(col.mean)\n",
    "    else:\n",
    "        return col.fillna(col.median)\n",
    "\n",
    "\n",
    "def solve_missing_values(df):\n",
    "    # if missing values < 5% => clip\n",
    "    # if missing values >= 5% => replace with median\n",
    "    # if missing values > 40% => drop column\n",
    "\n",
    "    if not has_missing_values(df):\n",
    "        print(\"there are no missing values\")\n",
    "        return df\n",
    "    else:\n",
    "        for column in df:\n",
    "            total_rows = len(df)\n",
    "            na_amount = count_missing_values(df[column])\n",
    "            na_percentage = (na_amount/total_rows) * 100\n",
    "            print(f'percentage: {column}: {na_percentage}%')\n",
    "\n",
    "            # the missing values are making smaller amount than 5% so we clip them\n",
    "            if na_percentage < 5:\n",
    "                df = df.dropna(subset=[column])\n",
    "            # the missing values will be replaced\n",
    "            elif na_percentage >= 5 and na_percentage <= 40:\n",
    "                df[column] = replace_missing_values(df[column])\n",
    "            # there is too much missing values, lets drop the column\n",
    "            else:\n",
    "                df.drop(column, axis=1, inplace=True)\n",
    "                print(f'dropped: {column}')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are no missing values\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>imei</th>\n",
       "      <th>mwra</th>\n",
       "      <th>c.android.youtube</th>\n",
       "      <th>c.katana</th>\n",
       "      <th>c.dogalize</th>\n",
       "      <th>c.android.chrome</th>\n",
       "      <th>c.android.gm</th>\n",
       "      <th>c.UCMobile.x86</th>\n",
       "      <th>c.raider</th>\n",
       "      <th>...</th>\n",
       "      <th>p.katana</th>\n",
       "      <th>p.google</th>\n",
       "      <th>p.notifier</th>\n",
       "      <th>p.android.defcontainer</th>\n",
       "      <th>p.android.vending</th>\n",
       "      <th>p.inputmethod.latin</th>\n",
       "      <th>p.simulator</th>\n",
       "      <th>p.olauncher</th>\n",
       "      <th>p.browser.provider</th>\n",
       "      <th>p.gms.persistent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-05 10:00:00</td>\n",
       "      <td>3590433799317661057</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.57832</td>\n",
       "      <td>15.81566</td>\n",
       "      <td>11.44041</td>\n",
       "      <td>7.54471</td>\n",
       "      <td>8.14947</td>\n",
       "      <td>74.82810</td>\n",
       "      <td>98.66217</td>\n",
       "      <td>...</td>\n",
       "      <td>58.68759</td>\n",
       "      <td>46.55151</td>\n",
       "      <td>51.61852</td>\n",
       "      <td>36.47510</td>\n",
       "      <td>17.56601</td>\n",
       "      <td>87.35626</td>\n",
       "      <td>32.00305</td>\n",
       "      <td>69.51890</td>\n",
       "      <td>69.85184</td>\n",
       "      <td>21.51831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-05 10:01:00</td>\n",
       "      <td>3590433799317661073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.53473</td>\n",
       "      <td>5.10559</td>\n",
       "      <td>13.23860</td>\n",
       "      <td>9.54599</td>\n",
       "      <td>12.96218</td>\n",
       "      <td>94.21770</td>\n",
       "      <td>33.36502</td>\n",
       "      <td>...</td>\n",
       "      <td>45.16131</td>\n",
       "      <td>34.43891</td>\n",
       "      <td>63.83929</td>\n",
       "      <td>72.42246</td>\n",
       "      <td>54.04621</td>\n",
       "      <td>35.68114</td>\n",
       "      <td>44.97550</td>\n",
       "      <td>84.71218</td>\n",
       "      <td>8.04137</td>\n",
       "      <td>76.74900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-05-05 10:02:00</td>\n",
       "      <td>863033069630348099</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.72129</td>\n",
       "      <td>13.76187</td>\n",
       "      <td>11.27464</td>\n",
       "      <td>16.56377</td>\n",
       "      <td>12.07414</td>\n",
       "      <td>73.70492</td>\n",
       "      <td>58.77622</td>\n",
       "      <td>...</td>\n",
       "      <td>59.91103</td>\n",
       "      <td>35.04185</td>\n",
       "      <td>32.01296</td>\n",
       "      <td>76.63106</td>\n",
       "      <td>38.55901</td>\n",
       "      <td>34.45914</td>\n",
       "      <td>86.26472</td>\n",
       "      <td>66.72859</td>\n",
       "      <td>75.20188</td>\n",
       "      <td>49.75180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-05-05 10:03:00</td>\n",
       "      <td>359043379931766007</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.17165</td>\n",
       "      <td>12.23936</td>\n",
       "      <td>8.53245</td>\n",
       "      <td>12.92046</td>\n",
       "      <td>12.97893</td>\n",
       "      <td>96.94804</td>\n",
       "      <td>65.95054</td>\n",
       "      <td>...</td>\n",
       "      <td>38.57900</td>\n",
       "      <td>38.61052</td>\n",
       "      <td>31.39066</td>\n",
       "      <td>16.12547</td>\n",
       "      <td>4.37465</td>\n",
       "      <td>90.40601</td>\n",
       "      <td>62.42150</td>\n",
       "      <td>18.49800</td>\n",
       "      <td>14.90605</td>\n",
       "      <td>14.40552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-05 10:04:00</td>\n",
       "      <td>8630330696303482071</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.52559</td>\n",
       "      <td>12.21164</td>\n",
       "      <td>10.68598</td>\n",
       "      <td>11.10452</td>\n",
       "      <td>11.09764</td>\n",
       "      <td>72.91827</td>\n",
       "      <td>69.47889</td>\n",
       "      <td>...</td>\n",
       "      <td>59.85793</td>\n",
       "      <td>50.84054</td>\n",
       "      <td>55.76454</td>\n",
       "      <td>48.84111</td>\n",
       "      <td>73.05974</td>\n",
       "      <td>97.77579</td>\n",
       "      <td>64.18499</td>\n",
       "      <td>78.20817</td>\n",
       "      <td>75.25798</td>\n",
       "      <td>0.55074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15421</th>\n",
       "      <td>2018-05-13 20:57:00</td>\n",
       "      <td>8630330696303481008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.17673</td>\n",
       "      <td>10.07652</td>\n",
       "      <td>14.13618</td>\n",
       "      <td>14.82835</td>\n",
       "      <td>15.14615</td>\n",
       "      <td>78.19613</td>\n",
       "      <td>68.83968</td>\n",
       "      <td>...</td>\n",
       "      <td>50.82987</td>\n",
       "      <td>57.03723</td>\n",
       "      <td>60.99230</td>\n",
       "      <td>68.80162</td>\n",
       "      <td>52.96995</td>\n",
       "      <td>75.72684</td>\n",
       "      <td>42.28340</td>\n",
       "      <td>51.32429</td>\n",
       "      <td>95.70857</td>\n",
       "      <td>69.73254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15422</th>\n",
       "      <td>2018-05-14 01:17:00</td>\n",
       "      <td>3590433799317662188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.96258</td>\n",
       "      <td>10.55080</td>\n",
       "      <td>11.27594</td>\n",
       "      <td>7.88137</td>\n",
       "      <td>10.30128</td>\n",
       "      <td>85.66285</td>\n",
       "      <td>24.44919</td>\n",
       "      <td>...</td>\n",
       "      <td>60.65804</td>\n",
       "      <td>53.67259</td>\n",
       "      <td>60.48345</td>\n",
       "      <td>44.49713</td>\n",
       "      <td>0.00498</td>\n",
       "      <td>82.00950</td>\n",
       "      <td>87.56227</td>\n",
       "      <td>88.34858</td>\n",
       "      <td>89.90047</td>\n",
       "      <td>21.42181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15423</th>\n",
       "      <td>2018-05-14 01:17:00</td>\n",
       "      <td>3590433799317662188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.96258</td>\n",
       "      <td>10.55080</td>\n",
       "      <td>11.27594</td>\n",
       "      <td>7.88137</td>\n",
       "      <td>10.30128</td>\n",
       "      <td>85.66285</td>\n",
       "      <td>24.44919</td>\n",
       "      <td>...</td>\n",
       "      <td>60.65804</td>\n",
       "      <td>53.67259</td>\n",
       "      <td>60.48345</td>\n",
       "      <td>44.49713</td>\n",
       "      <td>0.00498</td>\n",
       "      <td>82.00950</td>\n",
       "      <td>87.56227</td>\n",
       "      <td>88.34858</td>\n",
       "      <td>89.90047</td>\n",
       "      <td>21.42181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15424</th>\n",
       "      <td>2018-05-10 23:25:00</td>\n",
       "      <td>3590433799317661198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.02198</td>\n",
       "      <td>17.49888</td>\n",
       "      <td>14.75037</td>\n",
       "      <td>13.74212</td>\n",
       "      <td>10.38747</td>\n",
       "      <td>90.19126</td>\n",
       "      <td>5.84256</td>\n",
       "      <td>...</td>\n",
       "      <td>84.35493</td>\n",
       "      <td>53.06551</td>\n",
       "      <td>57.39217</td>\n",
       "      <td>36.38532</td>\n",
       "      <td>43.43760</td>\n",
       "      <td>51.47962</td>\n",
       "      <td>31.20808</td>\n",
       "      <td>67.90144</td>\n",
       "      <td>9.79193</td>\n",
       "      <td>38.18186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15425</th>\n",
       "      <td>2018-05-10 23:25:00</td>\n",
       "      <td>3590433799317661198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.02198</td>\n",
       "      <td>17.49888</td>\n",
       "      <td>14.75037</td>\n",
       "      <td>13.74212</td>\n",
       "      <td>10.38747</td>\n",
       "      <td>90.19126</td>\n",
       "      <td>5.84256</td>\n",
       "      <td>...</td>\n",
       "      <td>84.35493</td>\n",
       "      <td>53.06551</td>\n",
       "      <td>57.39217</td>\n",
       "      <td>36.38532</td>\n",
       "      <td>43.43760</td>\n",
       "      <td>51.47962</td>\n",
       "      <td>31.20808</td>\n",
       "      <td>67.90144</td>\n",
       "      <td>9.79193</td>\n",
       "      <td>38.18186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15426 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ts                 imei  mwra  c.android.youtube  \\\n",
       "0     2018-05-05 10:00:00  3590433799317661057   1.0           12.57832   \n",
       "1     2018-05-05 10:01:00  3590433799317661073   0.0           14.53473   \n",
       "2     2018-05-05 10:02:00   863033069630348099   1.0           14.72129   \n",
       "3     2018-05-05 10:03:00   359043379931766007   1.0           14.17165   \n",
       "4     2018-05-05 10:04:00  8630330696303482071   1.0           14.52559   \n",
       "...                   ...                  ...   ...                ...   \n",
       "15421 2018-05-13 20:57:00  8630330696303481008   1.0           17.17673   \n",
       "15422 2018-05-14 01:17:00  3590433799317662188   0.0           15.96258   \n",
       "15423 2018-05-14 01:17:00  3590433799317662188   0.0           15.96258   \n",
       "15424 2018-05-10 23:25:00  3590433799317661198   1.0           13.02198   \n",
       "15425 2018-05-10 23:25:00  3590433799317661198   1.0           13.02198   \n",
       "\n",
       "       c.katana  c.dogalize  c.android.chrome  c.android.gm  c.UCMobile.x86  \\\n",
       "0      15.81566    11.44041           7.54471       8.14947        74.82810   \n",
       "1       5.10559    13.23860           9.54599      12.96218        94.21770   \n",
       "2      13.76187    11.27464          16.56377      12.07414        73.70492   \n",
       "3      12.23936     8.53245          12.92046      12.97893        96.94804   \n",
       "4      12.21164    10.68598          11.10452      11.09764        72.91827   \n",
       "...         ...         ...               ...           ...             ...   \n",
       "15421  10.07652    14.13618          14.82835      15.14615        78.19613   \n",
       "15422  10.55080    11.27594           7.88137      10.30128        85.66285   \n",
       "15423  10.55080    11.27594           7.88137      10.30128        85.66285   \n",
       "15424  17.49888    14.75037          13.74212      10.38747        90.19126   \n",
       "15425  17.49888    14.75037          13.74212      10.38747        90.19126   \n",
       "\n",
       "       c.raider  ...  p.katana  p.google  p.notifier  p.android.defcontainer  \\\n",
       "0      98.66217  ...  58.68759  46.55151    51.61852                36.47510   \n",
       "1      33.36502  ...  45.16131  34.43891    63.83929                72.42246   \n",
       "2      58.77622  ...  59.91103  35.04185    32.01296                76.63106   \n",
       "3      65.95054  ...  38.57900  38.61052    31.39066                16.12547   \n",
       "4      69.47889  ...  59.85793  50.84054    55.76454                48.84111   \n",
       "...         ...  ...       ...       ...         ...                     ...   \n",
       "15421  68.83968  ...  50.82987  57.03723    60.99230                68.80162   \n",
       "15422  24.44919  ...  60.65804  53.67259    60.48345                44.49713   \n",
       "15423  24.44919  ...  60.65804  53.67259    60.48345                44.49713   \n",
       "15424   5.84256  ...  84.35493  53.06551    57.39217                36.38532   \n",
       "15425   5.84256  ...  84.35493  53.06551    57.39217                36.38532   \n",
       "\n",
       "       p.android.vending  p.inputmethod.latin  p.simulator  p.olauncher  \\\n",
       "0               17.56601             87.35626     32.00305     69.51890   \n",
       "1               54.04621             35.68114     44.97550     84.71218   \n",
       "2               38.55901             34.45914     86.26472     66.72859   \n",
       "3                4.37465             90.40601     62.42150     18.49800   \n",
       "4               73.05974             97.77579     64.18499     78.20817   \n",
       "...                  ...                  ...          ...          ...   \n",
       "15421           52.96995             75.72684     42.28340     51.32429   \n",
       "15422            0.00498             82.00950     87.56227     88.34858   \n",
       "15423            0.00498             82.00950     87.56227     88.34858   \n",
       "15424           43.43760             51.47962     31.20808     67.90144   \n",
       "15425           43.43760             51.47962     31.20808     67.90144   \n",
       "\n",
       "       p.browser.provider  p.gms.persistent  \n",
       "0                69.85184          21.51831  \n",
       "1                 8.04137          76.74900  \n",
       "2                75.20188          49.75180  \n",
       "3                14.90605          14.40552  \n",
       "4                75.25798           0.55074  \n",
       "...                   ...               ...  \n",
       "15421            95.70857          69.73254  \n",
       "15422            89.90047          21.42181  \n",
       "15423            89.90047          21.42181  \n",
       "15424             9.79193          38.18186  \n",
       "15425             9.79193          38.18186  \n",
       "\n",
       "[15426 rows x 33 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solve_missing_values(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(a):\n",
    "    lower = a.quantile(0.25) - 1.5 * stats.iqr(a)\n",
    "    upper = a.quantile(0.75) + 1.5 * stats.iqr(a)\n",
    "    \n",
    "    return a[(a > upper) | (a < lower)]\n",
    "\n",
    "\n",
    "def print_outliers_stat(df):\n",
    "    for column in df:\n",
    "        # Check if the column is numeric\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            outliers = identify_outliers(df[column])\n",
    "            print(f'{column}: {len(outliers)} => {(len(outliers) * 100) / len(df[column]):.2f}%')\n",
    "\n",
    "\n",
    "def remove_outliers(df):\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    for column in df_cleaned.select_dtypes(include=['number']).columns:\n",
    "        Q1 = df_cleaned[column].quantile(0.25)\n",
    "        Q3 = df_cleaned[column].quantile(0.75)\n",
    "        \n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        df_cleaned = df_cleaned[(df_cleaned[column] >= lower_bound) & (df_cleaned[column] <= upper_bound)]\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def replace_outliers(df):\n",
    "    df_replaced = df.copy()\n",
    "\n",
    "    for column in df_replaced.select_dtypes(include=['number']).columns:\n",
    "        Q1 = df_replaced[column].quantile(0.25)\n",
    "        Q3 = df_replaced[column].quantile(0.75)\n",
    "        \n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        df_replaced[column] = np.where(df_replaced[column] > upper_bound, upper_bound, df_replaced[column])\n",
    "        df_replaced[column] = np.where(df_replaced[column] < lower_bound, lower_bound, df_replaced[column])\n",
    "\n",
    "    return df_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processes_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# toto bude pravdepodobne do tela funkcie replace_outliers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# toto je tu kvoli warningu\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m processes_train[\u001b[39m'\u001b[39m\u001b[39mp.dogalize\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m processes_train[\u001b[39m'\u001b[39m\u001b[39mp.dogalize\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mfloat\u001b[39m)\n\u001b[1;32m      7\u001b[0m Q1 \u001b[39m=\u001b[39m processes_train[\u001b[39m'\u001b[39m\u001b[39mp.dogalize\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mquantile(\u001b[39m0.25\u001b[39m)\n\u001b[1;32m      8\u001b[0m Q3 \u001b[39m=\u001b[39m processes_train[\u001b[39m'\u001b[39m\u001b[39mp.dogalize\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mquantile(\u001b[39m0.75\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processes_train' is not defined"
     ]
    }
   ],
   "source": [
    "# toto bude pravdepodobne do tela funkcie replace_outliers\n",
    "\n",
    "# toto je tu kvoli warningu\n",
    "processes_train['p.dogalize'] = processes_train['p.dogalize'].astype(float)\n",
    "\n",
    "\n",
    "Q1 = processes_train['p.dogalize'].quantile(0.25)\n",
    "Q3 = processes_train['p.dogalize'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_whisker = Q1 - 1.5 * IQR\n",
    "upper_whisker = Q3 + 1.5 * IQR\n",
    "median_value = processes_train['p.dogalize'].median()\n",
    "\n",
    "processes_train.loc[(processes_train['p.dogalize'] < lower_whisker) |\n",
    "                  (processes_train['p.dogalize'] > upper_whisker)] = median_value\n",
    "\n",
    "processes_train = remove_outliers(processes_train)\n",
    "print_outliers_stat(processes_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kodovanie ketegorii do numerickeho formatu\n",
    "na enkodovanie kategorii by sme pouzili LabelEncoder pre atributy s malym poctom kategorii a OneHotEncoder pre viacero kategorii. Nase mergnute data su uz vsetky atributy numericke, takze toto netreba riesit v nasom pripade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged data\n",
    "Na merged datach vidime ze su vsetky atributy numericke a tak ich mozme vsetky pouzit v ML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (C-2b) Transformujte atribúty dát pre strojové učenie podľa dostupných techník minimálne: scaling (2 techniky), transformers (2 techniky) a ďalšie. Cieľom je aby ste testovali efekty a vhodne kombinovali v dátovom pipeline (od časti 2.3 a v 3. fáze). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import\n",
    "Importujeme potrebne kniznice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# odklad mwra\n",
    "Predtym ako budeme skalovat data si odlozime nasu predikovanu hodnotu mwra a v mergnutom datasete ju dropneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_mwra = merged_train_df['mwra']\n",
    "merged_train_with_mwra = merged_train_df\n",
    "merged_train_df = merged_train_df.drop(['mwra'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CELE SKALOVANIE A TRANSFORM JE ZLE, TREBA SKALOVAT MERGED_TRAIN_DF\n",
    "# A POTOM VYSLEDNY DF DAT DO TRANSFORMACIE A TAKTO VYTVORIT 4 KOMBINACIE A MAT 4 DATAFRAMY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. technika skalovania\n",
    "StandardScaler štandardizuje atribúty tak, že ich prevedie na distribúciu s priemerom 0 a štandardnou odchýlkou 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_standard = StandardScaler()\n",
    "scaled_data_standard = scaler_standard.fit_transform(merged_data)\n",
    "scaled_data_standard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. technika skalovania\n",
    "MinMaxScaler škáluje atribúty do zvoleného rozsahu, napríklad 0 až 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_minmax = MinMaxScaler()\n",
    "scaled_data_minmax = scaler_minmax.fit_transform(merged_data)\n",
    "scaled_data_minmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. technika transformers\n",
    "PowerTransformer využíva Box-Cox alebo Yeo-Johnson transformáciu na stabilizáciu rozdelenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_transformer = PowerTransformer(method='yeo-johnson')\n",
    "transformed_data_power = power_transformer.fit_transform(merged_data) # TODO: ze sem nepojde merged_data ale uz scalnute data\n",
    "transformed_data_power"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.technika transformers\n",
    "QuantileTransformer prevedie distribúciu dát na približne normálnu distribúciu pomocou kvantilov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_transformer = QuantileTransformer(output_distribution='normal')\n",
    "transformed_data_quantile = quantile_transformer.fit_transform(merged_data)\n",
    "transformed_data_quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tu by asi bolo fajn este pre kazdy df z tych 4 co budeme mat vykreslit spolocny graf\n",
    "# npr boxplot vsetkych atributov a pozriet sa ci su fajn rozlozene po transformaci a scalingu\n",
    "# \n",
    "# pripadne ak bude treba tak zmenit techniku transformovania (scaling techniky su vporiadku) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (D-1b) Zdôvodnite Vaše voľby/rozhodnutie pre realizáciu (t.j. zdokumentovanie)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!!! TODO !!!!!! TOTO ESTE SKONTROLOVAT A PREJST/PREPISAT\n",
    "\n",
    "# Zdokumentovanie a Zdôvodnenie Rozhodnutí pri Transformácii Atribútov\n",
    "\n",
    "Pri transformácii dát pre strojové učenie je dôležité zabezpečiť, aby atribúty mali vhodnú škálu, boli stabilné a aby ich distribúcia vyhovovala potrebám modelov. Nižšie je uvedené zdôvodnenie zvolených techník transformácie a škálovania.\n",
    "\n",
    "## 1. Scaling (Škálovanie)\n",
    "Výber techník škálovania bol motivovaný snahou o optimalizáciu výkonnosti modelov, ktoré často očakávajú normalizované alebo štandardizované vstupy.\n",
    "\n",
    "### 1.1 StandardScaler\n",
    "- **Dôvod**: StandardScaler bol zvolený, pretože štandardizuje atribúty na priemer 0 a štandardnú odchýlku 1, čo je vhodné pre algoritmy citlivé na rozsah dát, ako sú SVM, k-means alebo lineárna regresia.\n",
    "- **Výhoda**: Táto metóda udržuje štatistickú vlastnosť rozdelenia dát a dobre funguje pre dáta, ktoré už majú približne normálne rozdelenie.\n",
    "\n",
    "### 1.2 MinMaxScaler\n",
    "- **Dôvod**: MinMaxScaler bol vybraný pre prípady, kde bolo nutné previesť dáta do jednotného rozsahu, napríklad 0 až 1, čo je dôležité pri metódach, ako sú neurónové siete, kde normalizácia atribútov zlepšuje konvergenciu.\n",
    "- **Výhoda**: Škálovanie do konkrétneho rozsahu môže zvýšiť robustnosť algoritmu voči outlierom a zlepšiť interpretovateľnosť hodnoty každého atribútu.\n",
    "\n",
    "## 2. Transformers (Transformácie)\n",
    "Transformácie sme zvolili na stabilizáciu rozdelenia dát a odstránenie šikmosti (skewness), aby sa modely mohli lepšie zamerať na relevantné vzory.\n",
    "\n",
    "### 2.1 PowerTransformer\n",
    "- **Dôvod**: PowerTransformer bol zvolený pre svoje schopnosti stabilizovať rozdelenie dát pomocou Yeo-Johnson transformácie. Táto transformácia sa odporúča, ak sú atribúty pozitívne aj negatívne, pričom transformácia stabilizuje šikmosť a robí rozdelenie viac normálnym.\n",
    "- **Výhoda**: Zlepšenie normality môže pomôcť modelom lineárneho typu a optimalizáciám, ktoré sa opierajú o normálne rozdelenie dát, ako sú regresné modely a PCA.\n",
    "\n",
    "### 2.2 QuantileTransformer\n",
    "- **Dôvod**: QuantileTransformer bol zvolený pre svoju schopnosť transformovať rozdelenie dát na normálnu distribúciu bez ohľadu na pôvodné rozdelenie, čo je užitočné pri atribútoch s výraznými odchýlkami.\n",
    "- **Výhoda**: Táto metóda môže efektívne normalizovať dátové body a je vhodná pre modely citlivé na rozdelenie dát, ako sú algoritmy KNN alebo Bayesovské modely.\n",
    "\n",
    "## 3. Kombinácia Transformácií v Pipeline\n",
    "Pipeline bola zostavená na jednoduchšie testovanie a iteratívne kombinácie rôznych techník. Kombinovanie krokov zjednodušuje prácu a zabezpečuje konzistentnú predprípravu dát pre modelovanie.\n",
    "\n",
    "- **Výhoda pipeline**: Pipeline podporuje opakovateľnosť a konzistenciu v prístupe k predspracovaniu dát. Týmto spôsobom môžeme efektívne otestovať rôzne kombinácie transformácií, vybrať najlepšie varianty a následne ich jednoducho aplikovať na nové dáta alebo testovaciu množinu.\n",
    "\n",
    "## Celkové zdôvodnenie\n",
    "Tieto voľby boli zamerané na optimalizáciu dát pre rôzne modely strojového učenia, od lineárnych metód po algoritmy citlivé na rozsah a rozdelenie dát. Skúmanie viacerých techník nám umožňuje prispôsobiť transformáciu atribútov tak, aby maximalizovala výkonnosť a presnosť modelu, čo je cieľom tejto fázy prípravy dát.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Výber atribútov pre strojové učenie (5b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (A-3b) Zistite, ktoré atribúty (features) vo vašich dátach pre ML sú informatívne k predikovanej premennej (minimálne 3 techniky s porovnaním medzi sebou). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sem treba pridat tie 4 dataframes a na nich spravit 3 techniky identifikacie vztahu"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. technika Korelacna matica\n",
    "orelačná matica identifikuje silu vzťahu medzi každým atribútom a cieľovou premennou. Pre číselné atribúty sa často využíva Pearsonova korelácia; vysoká hodnota (kladná alebo záporná) naznačuje informatívnosť atribútu pre predikciu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vytvorenie korelačnej matice\n",
    "correlation_matrix = merged_data_with_mwra.corr()\n",
    "\n",
    "# Vizualizácia korelačnej matice pre zistenie vzťahu k cieľovej premennej mwra\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", center=0)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "korelacna matica je velmi velka, neprehladna a tazko citatelna, preto si vyberieme top 5 najvyssich hodnot k predikovanej premennej mwra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Výber korelácie pre cieľový stĺpec 'mwra' a zoradenie podľa absolútnej hodnoty\n",
    "top_5_features = correlation_matrix['mwra'].sort_values(key=abs, ascending=False).head(6)  # 'mwra' bude mať koreláciu 1 so sebou\n",
    "\n",
    "# Odstránenie 'mwra' zo zoznamu\n",
    "top_5_features = top_5_features.drop('mwra')\n",
    "\n",
    "# Zobrazenie top 5 atribútov\n",
    "print(top_5_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vysledok korelacnej matice mozme vidiet top 5 atributov ktore pravdepodobne vplyvaju na atribut mwra"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. technika Random Forest\n",
    "Použitím modelu Random Forest môžeme zistiť význam atribútov podľa toho, ako často a s akým dopadom sa daný atribút používa pri delení uzlov v rozhodovacích stromoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# rozdelenie dataframu na dataframe ktory obsahuje iba mwra\n",
    "# a taky v ktorom su ostatne atributy\n",
    "X = merged_data_with_mwra.drop('mwra', axis=1)\n",
    "y = merged_data_with_mwra['mwra']\n",
    "\n",
    "# Tréning Random Forest modelu\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Zobrazenie významu atribútov\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Vizualizácia významu atribútov\n",
    "feature_importances.plot(kind='barh', x='feature', y='importance', legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vysledok random forest mozme vidiet ze na predikciu mwra pravdepodobne vplyvaju atributy (od spodu nahor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. technika Selektivna metoda s ANOVA\n",
    "SelectKBest vyberá atribúty podľa štatistických testov. Pre klasifikačné úlohy sa často používa ANOVA alebo Chi-Square (Chi2) test. Táto metóda umožňuje identifikovať atribúty s najvyššou variabilitou voči cieľovej premennej. My pouzijeme Anovu pretoze mame numericke atributy, chi by sme pouzili pri kategorickych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Použitie ANOVA pre numerické atribúty\n",
    "selector = SelectKBest(score_func=f_classif, k='all')\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Získanie skóre a zoradenie atribútov podľa informatívnosti\n",
    "scores = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'score': selector.scores_\n",
    "}).sort_values(by='score', ascending=False)\n",
    "\n",
    "# Vizualizácia najlepších atribútov podľa SelectKBest\n",
    "scores.plot(kind='barh', x='feature', y='score', legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vysledok ANOVY mozme vidiet ze na predikciu mwra pravdepodobne vplyvaju atributy (od spodu nahor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS problemy\n",
    "Vidime warningy ohladom konstantnych hodnotach a problem s delenim. Atributy na ktore sa libka odkazuje su rok a mesiac, takze atributy ktore sme pridali z ts. kezde je s nimi problem a uz v tomto bode si myslime ze nebudu nijak prispievat na predikovanie mwra, tak ich nebudeme na zaciatku ani pridavat do dataframu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spájanie výsledkov do jedného DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'correlation_score': correlation_matrix['mwra'].abs().drop('mwra').values,  # Korelácia s cieľovou premennou\n",
    "    'random_forest_importance': rf_model.feature_importances_,  # Významnosť podľa Random Forest\n",
    "    'selectkbest_score': selector.scores_  # Významnosť podľa SelectKBest\n",
    "})\n",
    "\n",
    "# Normalizácia skóre na jednotnú mierku\n",
    "importance_df['correlation_score'] = importance_df['correlation_score'] / importance_df['correlation_score'].max()\n",
    "importance_df['random_forest_importance'] = importance_df['random_forest_importance'] / importance_df['random_forest_importance'].max()\n",
    "importance_df['selectkbest_score'] = importance_df['selectkbest_score'] / importance_df['selectkbest_score'].max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vypocet dolezitosti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priemerné skóre ako vážená dôležitosť atribútu\n",
    "importance_df['average_importance'] = importance_df[['correlation_score', 'random_forest_importance', 'selectkbest_score']].mean(axis=1)\n",
    "\n",
    "importance_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porovnanie vysledkov Korelacnej matice, Random forest a ANOVA testingu neformalne\n",
    "\n",
    "todo : "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (B-1b) Zoraďte zistené atribúty v poradí podľa dôležitosti. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# formalne hodnotenie dolezitosti atributov\n",
    "\n",
    "Zoradíme atribúty podľa vypočítanej priemernej dôležitosti od najdôležitejších po najmenej dôležité. Tieto atribúty predstavujú finálny výber najinformatívnejších premenných.\n",
    "\n",
    "Ako sme uz vysie spominali, vyberieme prve tri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoradenie podľa priemernej dôležitosti\n",
    "importance_df = importance_df.sort_values(by='average_importance', ascending=False)\n",
    "\n",
    "# Zobrazenie výsledkov\n",
    "print(importance_df[['feature', 'average_importance']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. p.android.gm\n",
    "2. p.android.documentsui\n",
    "3. c.katana\n",
    "\n",
    "Týmto spôsobom sme zoradili atribúty podľa ich dôležitosti pre cieľovú premennú. Atribúty s najvyšším skóre by mali byť zahrnuté v ďalšom modelovaní, pričom menej dôležité atribúty môžeme odstrániť alebo im prideliť nižšiu prioritu v ďalších analýzach."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (C-1b) Zdôvodnite Vaše voľby/rozhodnutie pre realizáciu (t.j. zdokumentovanie)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!! TODO !!! TOTO ESTE SKONTROLOVAT ALEBO NEJAK PREPISAT VLASTNYMI SLOVAMI\n",
    "\n",
    "# Zdokumentovanie a Zdôvodnenie Rozhodnutí pri Výbere a Zoradení Atribútov\n",
    "\n",
    "Pri výbere a zoradení atribútov sme použili rôzne metódy, aby sme zabezpečili robustnosť výsledkov a optimalizovali model pre predikciu. Nižšie sú uvedené hlavné dôvody výberu použitých techník a prístupu k zoradeniu atribútov.\n",
    "\n",
    "## 1. Kombinácia Viacerých Techník pre Výber Atribútov\n",
    "- **Dôvod**: Každá metóda výberu atribútov má svoje špecifiká a klady, a preto kombinácia viacerých techník znižuje riziko nesprávneho výberu a zvýrazňuje atribúty, ktoré sú konzistentne dôležité naprieč rôznymi metódami.\n",
    "- **Prístup**: Použili sme korelačnú maticu, ktorá poskytuje intuitívny a rýchly prehľad o vzťahoch medzi atribútmi a cieľovou premennou. Random Forest a SelectKBest sme pridali, pretože poskytujú pokročilejšie metódy hodnotenia dôležitosti, pričom využívajú silné stránky štatistických testov a stromových algoritmov.\n",
    "\n",
    "## 2. Normalizácia a Výpočet Priemernej Dôležitosti\n",
    "- **Dôvod**: Každá metóda používa rôzne mierky dôležitosti, preto sme výsledky zjednotili cez normalizáciu, aby boli porovnateľné. Výpočet priemernej dôležitosti umožňuje kombinovať výsledky z rôznych metód do jedného koherentného skóre, čo nám dáva spoľahlivý základ na zoradenie atribútov.\n",
    "- **Prístup**: Normalizované skóre každého atribútu z jednotlivých metód sme použili na výpočet priemernej dôležitosti. Táto metóda zaručuje, že naše konečné poradie nie je ovplyvnené extrémami v jednej z metód, a súčasne zohľadňuje viaceré aspekty dôležitosti.\n",
    "\n",
    "## 3. Zoradenie Atribútov na základe Priemernej Dôležitosti\n",
    "- **Dôvod**: Zoradenie podľa priemernej dôležitosti umožňuje zamerať sa na najvýznamnejšie atribúty pri vytváraní modelu, čo vedie k efektívnejšiemu a presnejšiemu modelu.\n",
    "- **Výhoda pre model**: Zahrnutím najdôležitejších atribútov zvyšujeme presnosť modelu a znižujeme šum v dátach. Na základe tohto poradia môžeme rozhodnúť o odstránení menej dôležitých atribútov, čím znížime riziko nadmerného fitovania a zvýšime generalizovateľnosť modelu.\n",
    "\n",
    "## Celkové Zdôvodnenie\n",
    "Tieto rozhodnutia boli navrhnuté s cieľom optimalizovať efektívnosť a interpretabilitu modelu. Použitím viacerých techník a váženého prístupu sme sa snažili zaistiť, že výber atribútov nie je založený na jednej metóde, ale zohľadňuje rozmanité prístupy. Týmto spôsobom sme schopní presnejšie identifikovať najinformatívnejšie atribúty, čo je kľúčové pre dosiahnutie kvalitných výsledkov v ďalšej fáze modelovania."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Replikovateľnosť predspracovania (5b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (A-3b) Upravte váš kód realizujúci predspracovanie trénovacej množiny tak, aby ho bolo možné bez ďalších úprav znovu použiť **na predspracovanie testovacej množiny** v kontexte strojového učenia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesing(df):\n",
    "    # prebehne preprocesing dataframu\n",
    "    # duplikaty -> funkcia ktora odstrani duplikaty\n",
    "    # missing values -> funkcia ktora ak bude menej ako 5% tak clip, ak viac tak nahradit meanom ak viac ako 40% drop cely column\n",
    "    # outliers -> \n",
    "    # scaling\n",
    "    # transform\n",
    "    # return vyslednych preprocessed data\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (B-2b) Využite možnosti **sklearn.pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp | anoteher_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(data=merged_testing, remove_duplicates, ....)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
