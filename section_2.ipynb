{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Realizácia predspracovania dát (5b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (A-1b) Dáta si rozdeľte na trénovaciu a testovaciu množinu podľa vami preddefinovaného pomeru. Ďalej pracujte len **s trénovacím datasetom**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "folder = Path(\"./070\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nacitame data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections_df = pd.read_csv(folder / \"connections.csv\", delimiter=\"\\t\", parse_dates=['ts'])\n",
    "devices_df = pd.read_csv(folder / \"devices.csv\", delimiter=\"\\t\")\n",
    "processes_df = pd.read_csv(folder / \"processes.csv\", delimiter=\"\\t\", parse_dates=['ts'])\n",
    "profiles_df = pd.read_csv(folder / \"profiles.csv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zlucenie dat z roznych tabuliek\n",
    "\n",
    "vieme ze vsetky tabulky maju spolocny atribut imei, ten ale nie je unikatny takze iba na zaklade jeho nemozme tabulky spajat.\n",
    "\n",
    "unikatny kluc podla ktoreho by sme mohli tabulky spojit je imei + datetime + mwra. Ak spojime tabulky na zaklade tychto atributov dostaneme validny merge\n",
    "\n",
    "kedze tieto atributy najdeme len v datach connections.csv a processes.csv, budeme joinovat iba tieto tabulky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data rozdelime pomocou funkcie train_test_split z sklearn.model v pomere 80:20, random_state zaistuje opakovatelnost aby sme mohli pripadne debugovat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(connections_df, processes_df, on=['imei', 'ts', 'mwra'], how='inner')\n",
    "\n",
    "# este rozdelime cas na konkretne atributy, aby\n",
    "# sme ich pripadne mohli pouzit v ML\n",
    "\n",
    "# merged_data['year'] = merged_data['ts'].dt.year\n",
    "# merged_data['month'] = merged_data['ts'].dt.month\n",
    "# merged_data['day'] = merged_data['ts'].dt.day\n",
    "# merged_data['hour'] = merged_data['ts'].dt.hour\n",
    "# merged_data['day_of_week'] = merged_data['ts'].dt.dayofweek\n",
    "\n",
    "# tieto data sme sa rozhodli nepridavat (vid 2.2 A))\n",
    "\n",
    "print(merged_data.shape)\n",
    "merged_data.head()\n",
    "\n",
    "\n",
    "# potom splitnut merged data na 80 % trainning a 20% testing pomocou train_test_split\n",
    "merged_train_df, merged_test_df = train_test_split(merged_data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (B-1b) Transformujte dáta na vhodný formát pre ML t.j. jedno pozorovanie musí byť opísané jedným riadkom a každý atribút musí byť v numerickom formáte (encoding). Iteratívne integrujte aj kroky v predspracovaní dát z prvej fázy (missing values, outlier detection) ako celok. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ako prve si v mergnutom datasete urcime spravne datove typy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: osetrit typy ak treba (asi nebude treba tak len napisat ze to je osetrene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identifikujeme a vyriesime duplikaty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    #TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## identifikujeme a vyriesime chybajuce hodnoty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missing_values(column):\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "def remove_missing_values(df):\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "\n",
    "def replace_missing_values(df):\n",
    "    #TODO\n",
    "    pass\n",
    "\n",
    "\n",
    "def solve_missing_values(df):\n",
    "    #TODO: if missing values < 5% => clip\n",
    "    # if missing values > 5% => replace with median\n",
    "    # if missing values > 40% => drop column\n",
    "    pass\n",
    "\n",
    "\n",
    "#funkcia ktore pozrie ktore data maju missing values\n",
    "def has_missing_values(df):\n",
    "    return df.isnull().any().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(a):\n",
    "    lower = a.quantile(0.25) - 1.5 * stats.iqr(a)\n",
    "    upper = a.quantile(0.75) + 1.5 * stats.iqr(a)\n",
    "    \n",
    "    return a[(a > upper) | (a < lower)]\n",
    "\n",
    "\n",
    "def print_outliers_stat(df):\n",
    "    for column in df:\n",
    "        # Check if the column is numeric\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            outliers = identify_outliers(df[column])\n",
    "            print(f'{column}: {len(outliers)} => {(len(outliers) * 100) / len(df[column]):.2f}%')\n",
    "\n",
    "\n",
    "def remove_outliers(df):\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    for column in df_cleaned.select_dtypes(include=['number']).columns:\n",
    "        Q1 = df_cleaned[column].quantile(0.25)\n",
    "        Q3 = df_cleaned[column].quantile(0.75)\n",
    "        \n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        df_cleaned = df_cleaned[(df_cleaned[column] >= lower_bound) & (df_cleaned[column] <= upper_bound)]\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def replace_outliers(df):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toto bude pravdepodobne do tela funkcie replace_outliers\n",
    "\n",
    "# toto je tu kvoli warningu\n",
    "processes_train['p.dogalize'] = processes_train['p.dogalize'].astype(float)\n",
    "\n",
    "\n",
    "Q1 = processes_train['p.dogalize'].quantile(0.25)\n",
    "Q3 = processes_train['p.dogalize'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_whisker = Q1 - 1.5 * IQR\n",
    "upper_whisker = Q3 + 1.5 * IQR\n",
    "median_value = processes_train['p.dogalize'].median()\n",
    "\n",
    "processes_train.loc[(processes_train['p.dogalize'] < lower_whisker) |\n",
    "                  (processes_train['p.dogalize'] > upper_whisker)] = median_value\n",
    "\n",
    "processes_train = remove_outliers(processes_train)\n",
    "print_outliers_stat(processes_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kodovanie ketegorii do numerickeho formatu\n",
    "na enkodovanie kategorii by sme pouzili LabelEncoder pre atributy s malym poctom kategorii a OneHotEncoder pre viacero kategorii. Nase mergnute data su uz vsetky atributy numericke, takze toto netreba riesit v nasom pripade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged data\n",
    "Na merged datach vidime ze su vsetky atributy numericke a tak ich mozme vsetky pouzit v ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (C-2b) Transformujte atribúty dát pre strojové učenie podľa dostupných techník minimálne: scaling (2 techniky), transformers (2 techniky) a ďalšie. Cieľom je aby ste testovali efekty a vhodne kombinovali v dátovom pipeline (od časti 2.3 a v 3. fáze). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import\n",
    "Importujeme potrebne kniznice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# odklad mwra\n",
    "Predtym ako budeme skalovat data si odlozime nasu predikovanu hodnotu mwra a v mergnutom datasete ju dropneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_mwra = merged_train_df['mwra']\n",
    "merged_train_with_mwra = merged_train_df\n",
    "merged_train_df = merged_train_df.drop(['mwra'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: CELE SKALOVANIE A TRANSFORM JE ZLE, TREBA SKALOVAT MERGED_TRAIN_DF\n",
    "# A POTOM VYSLEDNY DF DAT DO TRANSFORMACIE A TAKTO VYTVORIT 4 KOMBINACIE A MAT 4 DATAFRAMY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. technika skalovania\n",
    "StandardScaler štandardizuje atribúty tak, že ich prevedie na distribúciu s priemerom 0 a štandardnou odchýlkou 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_standard = StandardScaler()\n",
    "scaled_data_standard = scaler_standard.fit_transform(merged_data)\n",
    "scaled_data_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. technika skalovania\n",
    "MinMaxScaler škáluje atribúty do zvoleného rozsahu, napríklad 0 až 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_minmax = MinMaxScaler()\n",
    "scaled_data_minmax = scaler_minmax.fit_transform(merged_data)\n",
    "scaled_data_minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. technika transformers\n",
    "PowerTransformer využíva Box-Cox alebo Yeo-Johnson transformáciu na stabilizáciu rozdelenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_transformer = PowerTransformer(method='yeo-johnson')\n",
    "transformed_data_power = power_transformer.fit_transform(merged_data) # TODO: ze sem nepojde merged_data ale uz scalnute data\n",
    "transformed_data_power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.technika transformers\n",
    "QuantileTransformer prevedie distribúciu dát na približne normálnu distribúciu pomocou kvantilov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_transformer = QuantileTransformer(output_distribution='normal')\n",
    "transformed_data_quantile = quantile_transformer.fit_transform(merged_data)\n",
    "transformed_data_quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: tu by asi bolo fajn este pre kazdy df z tych 4 co budeme mat vykreslit spolocny graf\n",
    "# npr boxplot vsetkych atributov a pozriet sa ci su fajn rozlozene po transformaci a scalingu\n",
    "# \n",
    "# pripadne ak bude treba tak zmenit techniku transformovania (scaling techniky su vporiadku) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (D-1b) Zdôvodnite Vaše voľby/rozhodnutie pre realizáciu (t.j. zdokumentovanie)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!!! TODO !!!!!! TOTO ESTE SKONTROLOVAT A PREJST/PREPISAT\n",
    "\n",
    "# Zdokumentovanie a Zdôvodnenie Rozhodnutí pri Transformácii Atribútov\n",
    "\n",
    "Pri transformácii dát pre strojové učenie je dôležité zabezpečiť, aby atribúty mali vhodnú škálu, boli stabilné a aby ich distribúcia vyhovovala potrebám modelov. Nižšie je uvedené zdôvodnenie zvolených techník transformácie a škálovania.\n",
    "\n",
    "## 1. Scaling (Škálovanie)\n",
    "Výber techník škálovania bol motivovaný snahou o optimalizáciu výkonnosti modelov, ktoré často očakávajú normalizované alebo štandardizované vstupy.\n",
    "\n",
    "### 1.1 StandardScaler\n",
    "- **Dôvod**: StandardScaler bol zvolený, pretože štandardizuje atribúty na priemer 0 a štandardnú odchýlku 1, čo je vhodné pre algoritmy citlivé na rozsah dát, ako sú SVM, k-means alebo lineárna regresia.\n",
    "- **Výhoda**: Táto metóda udržuje štatistickú vlastnosť rozdelenia dát a dobre funguje pre dáta, ktoré už majú približne normálne rozdelenie.\n",
    "\n",
    "### 1.2 MinMaxScaler\n",
    "- **Dôvod**: MinMaxScaler bol vybraný pre prípady, kde bolo nutné previesť dáta do jednotného rozsahu, napríklad 0 až 1, čo je dôležité pri metódach, ako sú neurónové siete, kde normalizácia atribútov zlepšuje konvergenciu.\n",
    "- **Výhoda**: Škálovanie do konkrétneho rozsahu môže zvýšiť robustnosť algoritmu voči outlierom a zlepšiť interpretovateľnosť hodnoty každého atribútu.\n",
    "\n",
    "## 2. Transformers (Transformácie)\n",
    "Transformácie sme zvolili na stabilizáciu rozdelenia dát a odstránenie šikmosti (skewness), aby sa modely mohli lepšie zamerať na relevantné vzory.\n",
    "\n",
    "### 2.1 PowerTransformer\n",
    "- **Dôvod**: PowerTransformer bol zvolený pre svoje schopnosti stabilizovať rozdelenie dát pomocou Yeo-Johnson transformácie. Táto transformácia sa odporúča, ak sú atribúty pozitívne aj negatívne, pričom transformácia stabilizuje šikmosť a robí rozdelenie viac normálnym.\n",
    "- **Výhoda**: Zlepšenie normality môže pomôcť modelom lineárneho typu a optimalizáciám, ktoré sa opierajú o normálne rozdelenie dát, ako sú regresné modely a PCA.\n",
    "\n",
    "### 2.2 QuantileTransformer\n",
    "- **Dôvod**: QuantileTransformer bol zvolený pre svoju schopnosť transformovať rozdelenie dát na normálnu distribúciu bez ohľadu na pôvodné rozdelenie, čo je užitočné pri atribútoch s výraznými odchýlkami.\n",
    "- **Výhoda**: Táto metóda môže efektívne normalizovať dátové body a je vhodná pre modely citlivé na rozdelenie dát, ako sú algoritmy KNN alebo Bayesovské modely.\n",
    "\n",
    "## 3. Kombinácia Transformácií v Pipeline\n",
    "Pipeline bola zostavená na jednoduchšie testovanie a iteratívne kombinácie rôznych techník. Kombinovanie krokov zjednodušuje prácu a zabezpečuje konzistentnú predprípravu dát pre modelovanie.\n",
    "\n",
    "- **Výhoda pipeline**: Pipeline podporuje opakovateľnosť a konzistenciu v prístupe k predspracovaniu dát. Týmto spôsobom môžeme efektívne otestovať rôzne kombinácie transformácií, vybrať najlepšie varianty a následne ich jednoducho aplikovať na nové dáta alebo testovaciu množinu.\n",
    "\n",
    "## Celkové zdôvodnenie\n",
    "Tieto voľby boli zamerané na optimalizáciu dát pre rôzne modely strojového učenia, od lineárnych metód po algoritmy citlivé na rozsah a rozdelenie dát. Skúmanie viacerých techník nám umožňuje prispôsobiť transformáciu atribútov tak, aby maximalizovala výkonnosť a presnosť modelu, čo je cieľom tejto fázy prípravy dát.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Výber atribútov pre strojové učenie (5b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (A-3b) Zistite, ktoré atribúty (features) vo vašich dátach pre ML sú informatívne k predikovanej premennej (minimálne 3 techniky s porovnaním medzi sebou). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: sem treba pridat tie 4 dataframes a na nich spravit 3 techniky identifikacie vztahu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. technika Korelacna matica\n",
    "orelačná matica identifikuje silu vzťahu medzi každým atribútom a cieľovou premennou. Pre číselné atribúty sa často využíva Pearsonova korelácia; vysoká hodnota (kladná alebo záporná) naznačuje informatívnosť atribútu pre predikciu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vytvorenie korelačnej matice\n",
    "correlation_matrix = merged_data_with_mwra.corr()\n",
    "\n",
    "# Vizualizácia korelačnej matice pre zistenie vzťahu k cieľovej premennej mwra\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "korelacna matica je velmi velka, neprehladna a tazko citatelna, preto si vyberieme top 5 najvyssich hodnot k predikovanej premennej mwra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Výber korelácie pre cieľový stĺpec 'mwra' a zoradenie podľa absolútnej hodnoty\n",
    "top_5_features = correlation_matrix['mwra'].sort_values(key=abs, ascending=False).head(6)  # 'mwra' bude mať koreláciu 1 so sebou\n",
    "\n",
    "# Odstránenie 'mwra' zo zoznamu\n",
    "top_5_features = top_5_features.drop('mwra')\n",
    "\n",
    "# Zobrazenie top 5 atribútov\n",
    "print(top_5_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vysledok korelacnej matice mozme vidiet top 5 atributov ktore pravdepodobne vplyvaju na atribut mwra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. technika Random Forest\n",
    "Použitím modelu Random Forest môžeme zistiť význam atribútov podľa toho, ako často a s akým dopadom sa daný atribút používa pri delení uzlov v rozhodovacích stromoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# rozdelenie dataframu na dataframe ktory obsahuje iba mwra\n",
    "# a taky v ktorom su ostatne atributy\n",
    "X = merged_data_with_mwra.drop('mwra', axis=1)\n",
    "y = merged_data_with_mwra['mwra']\n",
    "\n",
    "# Tréning Random Forest modelu\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Zobrazenie významu atribútov\n",
    "feature_importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Vizualizácia významu atribútov\n",
    "feature_importances.plot(kind='barh', x='feature', y='importance', legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vysledok random forest mozme vidiet ze na predikciu mwra pravdepodobne vplyvaju atributy (od spodu nahor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. technika Selektivna metoda s ANOVA\n",
    "SelectKBest vyberá atribúty podľa štatistických testov. Pre klasifikačné úlohy sa často používa ANOVA alebo Chi-Square (Chi2) test. Táto metóda umožňuje identifikovať atribúty s najvyššou variabilitou voči cieľovej premennej. My pouzijeme Anovu pretoze mame numericke atributy, chi by sme pouzili pri kategorickych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Použitie ANOVA pre numerické atribúty\n",
    "selector = SelectKBest(score_func=f_classif, k='all')\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Získanie skóre a zoradenie atribútov podľa informatívnosti\n",
    "scores = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'score': selector.scores_\n",
    "}).sort_values(by='score', ascending=False)\n",
    "\n",
    "# Vizualizácia najlepších atribútov podľa SelectKBest\n",
    "scores.plot(kind='barh', x='feature', y='score', legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vysledok ANOVY mozme vidiet ze na predikciu mwra pravdepodobne vplyvaju atributy (od spodu nahor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS problemy\n",
    "Vidime warningy ohladom konstantnych hodnotach a problem s delenim. Atributy na ktore sa libka odkazuje su rok a mesiac, takze atributy ktore sme pridali z ts. kezde je s nimi problem a uz v tomto bode si myslime ze nebudu nijak prispievat na predikovanie mwra, tak ich nebudeme na zaciatku ani pridavat do dataframu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spájanie výsledkov do jedného DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'correlation_score': correlation_matrix['mwra'].abs().drop('mwra').values,  # Korelácia s cieľovou premennou\n",
    "    'random_forest_importance': rf_model.feature_importances_,  # Významnosť podľa Random Forest\n",
    "    'selectkbest_score': selector.scores_  # Významnosť podľa SelectKBest\n",
    "})\n",
    "\n",
    "# Normalizácia skóre na jednotnú mierku\n",
    "importance_df['correlation_score'] = importance_df['correlation_score'] / importance_df['correlation_score'].max()\n",
    "importance_df['random_forest_importance'] = importance_df['random_forest_importance'] / importance_df['random_forest_importance'].max()\n",
    "importance_df['selectkbest_score'] = importance_df['selectkbest_score'] / importance_df['selectkbest_score'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vypocet dolezitosti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priemerné skóre ako vážená dôležitosť atribútu\n",
    "importance_df['average_importance'] = importance_df[['correlation_score', 'random_forest_importance', 'selectkbest_score']].mean(axis=1)\n",
    "\n",
    "importance_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porovnanie vysledkov Korelacnej matice, Random forest a ANOVA testingu neformalne\n",
    "\n",
    "todo : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (B-1b) Zoraďte zistené atribúty v poradí podľa dôležitosti. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# formalne hodnotenie dolezitosti atributov\n",
    "\n",
    "Zoradíme atribúty podľa vypočítanej priemernej dôležitosti od najdôležitejších po najmenej dôležité. Tieto atribúty predstavujú finálny výber najinformatívnejších premenných.\n",
    "\n",
    "Ako sme uz vysie spominali, vyberieme prve tri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoradenie podľa priemernej dôležitosti\n",
    "importance_df = importance_df.sort_values(by='average_importance', ascending=False)\n",
    "\n",
    "# Zobrazenie výsledkov\n",
    "print(importance_df[['feature', 'average_importance']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. p.android.gm\n",
    "2. p.android.documentsui\n",
    "3. c.katana\n",
    "\n",
    "Týmto spôsobom sme zoradili atribúty podľa ich dôležitosti pre cieľovú premennú. Atribúty s najvyšším skóre by mali byť zahrnuté v ďalšom modelovaní, pričom menej dôležité atribúty môžeme odstrániť alebo im prideliť nižšiu prioritu v ďalších analýzach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (C-1b) Zdôvodnite Vaše voľby/rozhodnutie pre realizáciu (t.j. zdokumentovanie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!! TODO !!! TOTO ESTE SKONTROLOVAT ALEBO NEJAK PREPISAT VLASTNYMI SLOVAMI\n",
    "\n",
    "# Zdokumentovanie a Zdôvodnenie Rozhodnutí pri Výbere a Zoradení Atribútov\n",
    "\n",
    "Pri výbere a zoradení atribútov sme použili rôzne metódy, aby sme zabezpečili robustnosť výsledkov a optimalizovali model pre predikciu. Nižšie sú uvedené hlavné dôvody výberu použitých techník a prístupu k zoradeniu atribútov.\n",
    "\n",
    "## 1. Kombinácia Viacerých Techník pre Výber Atribútov\n",
    "- **Dôvod**: Každá metóda výberu atribútov má svoje špecifiká a klady, a preto kombinácia viacerých techník znižuje riziko nesprávneho výberu a zvýrazňuje atribúty, ktoré sú konzistentne dôležité naprieč rôznymi metódami.\n",
    "- **Prístup**: Použili sme korelačnú maticu, ktorá poskytuje intuitívny a rýchly prehľad o vzťahoch medzi atribútmi a cieľovou premennou. Random Forest a SelectKBest sme pridali, pretože poskytujú pokročilejšie metódy hodnotenia dôležitosti, pričom využívajú silné stránky štatistických testov a stromových algoritmov.\n",
    "\n",
    "## 2. Normalizácia a Výpočet Priemernej Dôležitosti\n",
    "- **Dôvod**: Každá metóda používa rôzne mierky dôležitosti, preto sme výsledky zjednotili cez normalizáciu, aby boli porovnateľné. Výpočet priemernej dôležitosti umožňuje kombinovať výsledky z rôznych metód do jedného koherentného skóre, čo nám dáva spoľahlivý základ na zoradenie atribútov.\n",
    "- **Prístup**: Normalizované skóre každého atribútu z jednotlivých metód sme použili na výpočet priemernej dôležitosti. Táto metóda zaručuje, že naše konečné poradie nie je ovplyvnené extrémami v jednej z metód, a súčasne zohľadňuje viaceré aspekty dôležitosti.\n",
    "\n",
    "## 3. Zoradenie Atribútov na základe Priemernej Dôležitosti\n",
    "- **Dôvod**: Zoradenie podľa priemernej dôležitosti umožňuje zamerať sa na najvýznamnejšie atribúty pri vytváraní modelu, čo vedie k efektívnejšiemu a presnejšiemu modelu.\n",
    "- **Výhoda pre model**: Zahrnutím najdôležitejších atribútov zvyšujeme presnosť modelu a znižujeme šum v dátach. Na základe tohto poradia môžeme rozhodnúť o odstránení menej dôležitých atribútov, čím znížime riziko nadmerného fitovania a zvýšime generalizovateľnosť modelu.\n",
    "\n",
    "## Celkové Zdôvodnenie\n",
    "Tieto rozhodnutia boli navrhnuté s cieľom optimalizovať efektívnosť a interpretabilitu modelu. Použitím viacerých techník a váženého prístupu sme sa snažili zaistiť, že výber atribútov nie je založený na jednej metóde, ale zohľadňuje rozmanité prístupy. Týmto spôsobom sme schopní presnejšie identifikovať najinformatívnejšie atribúty, čo je kľúčové pre dosiahnutie kvalitných výsledkov v ďalšej fáze modelovania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Replikovateľnosť predspracovania (5b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (A-3b) Upravte váš kód realizujúci predspracovanie trénovacej množiny tak, aby ho bolo možné bez ďalších úprav znovu použiť **na predspracovanie testovacej množiny** v kontexte strojového učenia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesing(df):\n",
    "    # prebehne preprocesing dataframu\n",
    "    # duplikaty -> funkcia ktora odstrani duplikaty\n",
    "    # missing values -> funkcia ktora ak bude menej ako 5% tak clip, ak viac tak nahradit meanom ak viac ako 40% drop cely column\n",
    "    # outliers -> \n",
    "    # scaling\n",
    "    # transform\n",
    "    # return vyslednych preprocessed data\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (B-2b) Využite možnosti **sklearn.pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp | anoteher_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(data=merged_testing, remove_duplicates, ....)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
